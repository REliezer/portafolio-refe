---
title: Creaci贸n de un Pipeline de Datos y API con Cach茅 Inteligente
resume: Soluci贸n integral de backend para migrar datos a gran escala, implementar APIs seguras y optimizadas, y desplegar todo en la nube.
date: 2025-07-19
imageCover: "@assets/projects/default.svg"
imageCoverAlt: Proyecto Sistemas Expertos
categories: [Proyectos Academicos]
platform:
  web: true
  movil: false
  nativo: false
  pwa: false
  desktop: false
  api: true
author: Rodrigo F煤nes
github:
  - title: "Terraform AmazonAPI"
    link: "https://github.com/REliezer/TerraformAmazonAPI"
  - title: "AmazonAPI"
    link: "https://github.com/REliezer/amazonAPI"
    
link: "https://api-amazonproducts-dev.azurewebsites.net/"

stack:
  - label: "Azure App Service Plans"
    group: "Backend"

  - label: "Azure App Service"
    group: "Backend"

  - label: "Azure SQL Server"
    group: "Base de Datos"

  - label: "Azure SQL Database"
    group: "Base de Datos"

  - label: "Azure Cache for Redis"
    group: "Base de Datos"

  - label: "Application Insights"
    group: "Monitoreo"

  - label: "Azure Resource Group"
    group: "Infraestructura y Administraci贸n"

  - label: "Terraform"
    group: "Infraestructura y Administraci贸n"

  - label: "Azure Data Factory"
    group: "Integraci贸n y Automatizaci贸n"

  - label: "Azure Container Registry"
    group: "Almacenamiento y Contenedores"

  - label: "Azure Blob Storage"
    group: "Almacenamiento y Contenedores"

  - label: "Docker"
    group: "Almacenamiento y Contenedores"

  - label: "Firebase Authentication"
    group: "Identidad y Seguridad"

description: |
  Desarrollo de una soluci贸n integral de backend capaz de migrar datos a gran escala, exponerlos mediante una API segura y optimizada, y desplegar todo en un entorno productivo en la nube.  
  Adem谩s, se implementa un sistema de monitoreo y una estrategia de cach茅 con invalidaci贸n autom谩tica.

content:
  description: "Para poder realizar este proyecto se tuvieron que ir completando diferentes fase."
  item:
  - nombre: " Fase 1: Preparaci贸n y Migraci贸n de Datos"
    resume: |
        En esta etapa primero se tenia que buscar un dataset que contuviera una cantidad considerable de registros y luego crear la base de datos y las tablas correspondientes en Azure SQL. Para nuestro proyecto se utiliz贸 un dataset de Amazon Products que fu茅 obtenido de Kaggle.
        Para hacer la migraci贸n de los datos se utiliza Azure Data Factory mediante la creaci贸n de pipelines que extraen los datos del dataset (que fue subido al Azure Blob Storage) hacia cada una de las tablas.
  - nombre: " Fase 2: Desarrollo de API y Autenticaci贸n"
    resume: |
        Con los datos cargados en las tablas de la base de datos se crean varias API para interactuar con ellos, aplicando buenas pr谩cticas de desarrollo y seguridad.
        Para desarrollar las API se utiliza FastAPI (Python) y en ciertos endpoints se requerira un token JWT, se implementa Firebase Authentication para gestionar el registro e inicio de sesi贸n de usuarios mediante correo electronico.
  - nombre: "诧 Fase 3: Monitoreo de Rendimiento"
    resume: |
        Se utiliza Azure Application Insights junto con las API para capturar telemetr铆a, trazas de solicitudes, tiempos de respuesta y posibles errores.
        Para ello se realiza m煤ltiples request consecutivos a los endpoints para analizar y observar el comportamiento de la API en el panel de Application Insights.
  - nombre: " Fase 4: Implementaci贸n de Cach茅 con Redis"
    resume: |
        Para mejorar los tiempos de respuesta, se implement贸 una capa de cach茅, para ello se Configura y conecta una base de datos de cache con Redis.
        En esta fase se tenia que generar una key para guardar cada respuesta en Redis, teniendo dos posibles escenarios:
    modulos:
      - Si la key existe, la respuesta se devuelve inmediatamente de esta.
      - Si no existe, se consulta la base de datos, se almacena la respuesta en Redis usando la key din谩mica y luego se devuelve la respuesta al usuario.
      
  - nombre: "Ч Fase 5: Invalidaci贸n de Cach茅"
    resume: |
        En esta parte se plantea el problema de que al ingresar nuevos datos, los datos originales cambiarian y dejar铆an el cach茅 obsoleto, por tanto el cach茅 generado debia ser 'inteligente'.
        Para ello se crea una funci贸n que cada vez que se agregaba un nuevo registro esta eliminaba la key espec铆fica en Redis que estaba relacionada a cada categor铆a.
  - nombre: " Fase 6: Despliegue en la Nube con Docker"
    resume: |
        La 煤ltima fase... La aplicaci贸n debia ser empaquetada y desplegada para funcionar en un entorno de nube real de forma independiente.
        Para ello se cre贸 un Dockerfile con todas las dependencias y configuraciones necesarias para que pudiera ejecutarse de forma aislada y la imagen de Docker se publico en el Azure Container Registry y se termin贸 con el despliegue del contenedor usando Azure App Service.
---

